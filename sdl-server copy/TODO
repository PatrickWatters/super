Create a global preftech queue and reference it from each job instead of 
the GroupBatch preftech queue.

When queueing items give a prioty value and (batch_id, group_id)

Have a number of consumer threads that are processing items on the queue. 
For each item pulled off from the queue:
If item is cached
    //how long has it been since it was last pinged? Over 15min? Ping it and update the batch metadata to reflect the new timestamp!!
If item is not cached:
    //how long until the job needs to access it i.e. its priorty score.
    less than 30 seconds? Invoke the function to create the batch!
    Update the batch details to reflect this!!

Note: there might be occasions when an item is showing up as cached, but has
been evicetd because it hasn't been pinged in a while.. I need to think about
this i.e. find a way to prevent this from happening, and/or, find an elegent way to handle it in the case that it does occur! (Possibly treat it as a normal cache miss from the Pytorch training job point of view). In-fact, I need to understand how to handle cache misses in general!

I also need to start thinking about exceeding the size of the SION cache and what happens in that scenario. We can always sping more functions I guess at no extra cost (this is why Serverless is good - storage is basically free!),
but what can I do to stop it if it does happen:
    - reduce the keep-alive-pings (remember SION has its own pint timeline which I could possibly disable)




#Possible Improvements:
- predict the delay before it happens based on whether it is a cache miss or not, this means tracking how long it takes
to load a bacth on a cache miss overtime and using that value to predict the delay and update the queue! Additionally,
we could handle cache misses within the service itself!

